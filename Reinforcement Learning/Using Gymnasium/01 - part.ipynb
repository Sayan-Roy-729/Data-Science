{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning ([FreeCodeCamp](https://www.youtube.com/watch?v=vufTSJbzKGU&t=32s))\n",
    "\n",
    "## Basics of Reinforcement Learning:\n",
    "\n",
    "1. Reinforcement Learning is a type of machine learning in which an agent learns to make decisions by interacting with an environment.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://gymnasium.farama.org/_images/AE_loop.png\" width=400 alt=\"Demo of the Reinforcement Learning\"/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. An agent is an entity that interacts with an environment in order to learn how to make decisions that will maximize a specific goal or objects. The agent can be thought of as an autonomous decision-making entity that receives inputs from the environment, perform actions, and receives rewards or penalties based on its actions.\n",
    "3. The environment, on the other hand, is the external system or context in which the agent operates. The environment can be a simulation, a physical system, or any other system that the agent interacts with. The environment provides feedback to the agent in the form of rewards or punishments, which the agent uses to learn how to make better decisions.\n",
    "4. The agent receives feedback in the form of rewards or penalties, and its goal is to maximize the total reward it receives over time.\n",
    "5. The agent follows a trial-and-error process of taking actions, observing the consequences, and adjusting its behavior based on the rewards it receives. Over time,the agent learns to identify the actions that lead to the highest rewards and avoid those that lead to penalties.\n",
    "6. RL agents can adapt to new situations and environments. They can adjust their behavior based on the feedback they receive and can continually improve their performance over time.\n",
    "7. RL agents can learn to optimize their behavior over time, leading to more efficient decision making and better performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Real-world use cases of RL:**\n",
    "- <u>**Robotics:**</u> Reinforcement learning can be used to train robots to perform tasks such as grasping objects, navigating in complex environments, and interacting with humans. For example, RL has been used to train robots to play table tennis, where the robot learns to adjust its movements based on the position of the ball.\n",
    "- <u>**Game Playing:**</u> Reinforcement learning has been applied to game playing, particularly in the development of artificial intelligence agents that can play games such as chess, Go and Atari games. For example, AlphaGo, a computer program developed by Google DeepMind, uses RL to learn to play the game of Go at a world-class level.\n",
    "- <u>**Autonomous Driving:**</u> Reinforcement learning can be used to train autonomous vehicles to make decisions in complex and dynamic environments. For example, RL can be used to train a self-driving car to navigate through traffic, avoid obstacles, and make safe and efficient driving decisions.\n",
    "- <u>**Personalized Recommendations:**</u> RL can be used to provide personalized recommendations to users based on their preferences and behavior. For example, RL can be used to optimize the recommendations of a video streaming service, learning what content to recommend to users to maximize user engagement and satisfaction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is **[Gymnasium](https://gymnasium.farama.org/)**?\n",
    "\n",
    "- It provides a collection of environments or \"tasks\" that can be used to test and develop reinforcement learning algorithms. These environments are typically game-like, with well defined rules and a reward structure, making them useful for evaluating and comparing different reinforcement learning algorithms.\n",
    "- The Gym toolkit includes a set of interfaces and tools for interacting with the environments, such as observation spaces, action spaces, and rewards. This make it easy to build and test reinforcement learning algorithms in a standardized way.\n",
    "- These environments range from simple games, like Pong or Breakout, to more complex simulations, like robotics or autonomous driving. Gymnasium environments are designed to be easy to use and come with a standard interface for interacting with the environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started with Gymnasium\n",
    "\n",
    "The basic process for using Gymnasium to train a Reinforcement Learning model is as follows:\n",
    "1. Define the environment you want to work with.\n",
    "2. Create an instance of the environment.\n",
    "3. Define the agent's policy (i.e., how it decides which action to take).\n",
    "4. Interact with the environment, taking actions and receiving rewards.\n",
    "5. Update the agent's policy based on the rewards it receives.\n",
    "\n",
    "### Main concepts of OpenAI Gymnasium\n",
    "\n",
    "1. <u>**Observation and Action Spaces:**</u> An observation space is the set of possible states that an agent can observe in the environment. An action space is the set of possible actions that an agent can take in the environment.\n",
    "2. <u>**Episode:**</u> An episode is a complete run-through of an environment, starting from the initial state and continuing until a terminal state is reached. Each episode is composed of a sequence of states, actions and rewards.\n",
    "3. <u>**Wrapper:**</u> A wrapper is a tool in OpenAI Gym that allows you to modify an environment's behavior without changing its code. Wrappers can be used to add features such as time limits, reward shaping and action masking.\n",
    "4. <u>**Benchmark:**</u> OpenAI Gym provides a set of benchmark environments, which are standardized tasks that can be used to evaluate and compare reinforcement learning algorithms. These benchmarks include classic control tasks, Atari games, and robotics tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/)\n",
    "\n",
    "<center>\n",
    "\n",
    "![Blackjack example](../Images/image-1.png)\n",
    "|\n",
    "</center>\n",
    "\n",
    "**Here are the basic rules of Blackjack:**\n",
    "1. The game is played with one or more decks of standard playing cards.\n",
    "2. Each player is dealt two cards, and the dealer is also dealt two cards, with one card face down.\n",
    "3. The value of each card is determined by its rank. Aces can be worth 1 or 11, face cards (kings, queens, and jacks) are worth 10, and all other cards are worth their face value.\n",
    "4. Players have the option to \"hit\" and take additional cards to improve their hand, or \"stand\" and keep their current hand.\n",
    "5. The dealer must hit until their hand has a value of 17 or more.\n",
    "6. If a player's hand goes over 21, they \"bust\" and lose the game.\n",
    "6. If the dealer's hand goes over 21, the player wins the game.\n",
    "7. If neither the player nor the dealer busts, the hand with the highest total value that is less than or equal to 21 wins the game.\n",
    "\n",
    "#### **Action Space**\n",
    "\n",
    "The action shape is (1,) in the range {0, 1} indicating whether to stick or hit.\n",
    "\n",
    "0: Stick\n",
    "\n",
    "1: Hit\n",
    "\n",
    "#### **Observation Space**\n",
    "\n",
    "The observation consists of a 3-tuple containing: the player’s current sum, the value of the dealer’s one showing card (1-10 where 1 is ace), and whether the player holds a usable ace (0 or 1).\n",
    "\n",
    "#### **Starting State**\n",
    "\n",
    "The starting state is initialized in the following range.\n",
    "\n",
    "#### <b>Observation</b>\n",
    "\n",
    "| **Observation** | **Min** | **Max** |\n",
    "| :--: | :--: | :--: |\n",
    "| Player current sum | 4 | 12 |\n",
    "| Dealer showing card value | 2 | 11 |\n",
    "| Usable Ace | 0 | 1 |\n",
    "\n",
    "#### **Rewards**\n",
    "\n",
    "win game: +1\n",
    "\n",
    "lose game: -1\n",
    "\n",
    "draw game: 0\n",
    "\n",
    "#### **Episode End**\n",
    "\n",
    "The episode ends if the following happens:\n",
    "\n",
    "**Termination:** \n",
    "- The player hits and the sum of hand exceeds 21.\n",
    "- The player sticks.\n",
    "- An ace will always be counted as usable (11) unless it busts the player.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
